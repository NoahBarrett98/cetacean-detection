{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intial preprocessor stores the arrays as .npz files, it seems that parquet could be more efficient, I will experiment with converting the array as parquet here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have an array with shape (290, 400, 400)\n",
    "# This represents 290 grayscale images of size 400Ã—400\n",
    "data_f = np.load(r\"C:\\Users\\NoahB\\OneDrive\\Desktop\\cetacean_detection\\nefsc_sbnms_200903_nopp6_ch10\\processed\\intial_run\\images\\NOPP6_EST_20090328_000000_CH10.npz\")\n",
    "images = data_f['X']\n",
    "labels = data_f['y']\n",
    "# Create a dictionary with each image as a row\n",
    "data = {\n",
    "    'labels': [l.tobytes() for l in labels],  # Image identifiers\n",
    "    'image_data': [img.tobytes() for img in images]\n",
    "}\n",
    "\n",
    "# Create a PyArrow table\n",
    "table = pa.Table.from_pydict(data)\n",
    "\n",
    "# Save as Parquet\n",
    "pq.write_table(table, 'images_collection.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46401650\n",
      "3770871\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.getsize(r\"nefsc_sbnms_200903_nopp6_ch10/processed/intial_run/images/NOPP6_EST_20090328_000000_CH10.npz\"))\n",
    "print(os.path.getsize(r'C:\\Users\\NoahB\\OneDrive\\Desktop\\cetacean_detection\\nefsc_sbnms_200903_nopp6_ch10\\processed\\intial_run\\parquet\\image_arrays.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a storage difference of 46mb to 3.6mb... no brainer to use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46403208\n"
     ]
    }
   ],
   "source": [
    "# experiment with hdf5\n",
    "import h5py\n",
    "data_f = r\"C:\\Users\\NoahB\\OneDrive\\Desktop\\cetacean_detection\\nefsc_sbnms_200903_nopp6_ch10\\processed\\intial_run\\hdf5\\sample.h5\"\n",
    "# Saving images\n",
    "with h5py.File(data_f, 'w') as f:\n",
    "    images = images\n",
    "    labels = labels\n",
    "    f.create_dataset('images', data=images)\n",
    "    f.create_dataset('labels', data=labels)\n",
    "    \n",
    "print(os.path.getsize(data_f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 does not yield the same data storage beenfits (its much larger), but it is much better for reading random indexes - we can index specific indexes in the array without loading in the entire dataset. SO while we are training locally, hdf5 will be the best option, as it will be easily shuffled and read (rather than having to preshuffle and store as a parquet file)\n",
    "* if we end up needing very quick upload / download of the entire dataset, then we may want to switch to parquet. For now the larger dataset should'nt be a huge issue. \n",
    "* final decision - use hdf5 for now for its reading capabilities. but swithc to parquet if we need super efficient file upload / download. \n",
    "\n",
    "The entire dataset on one hdf5 file is 29GB, this is the same as the entirety of the npy dataset, so they have similar storage sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (c_det)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
